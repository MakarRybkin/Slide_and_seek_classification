{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 91496,
     "databundleVersionId": 11802066,
     "sourceType": "competition"
    },
    {
     "sourceId": 11474913,
     "sourceType": "datasetVersion",
     "datasetId": 7191644
    }
   ],
   "dockerImageVersionId": 31012,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The solution to the competition https://zindi.africa/competitions/classification-for-landslide-detection demonstrates the process of creating and training a deep learning model for binary classification of landslides based on multispectral data and Sentinel-2.\n",
    "### Competition metric - f1"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Installing the comet_ml library. Comet ML is a platform for tracking, comparing, and optimizing machine learning experiments"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import necessary libraries\n",
    "!pip install comet_ml > /dev/null 2>&1"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import comet_ml\n",
    "COMET_API_KEY = \"My CometML\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.ndimage import rotate, shift, zoom\n",
    "data = \"/kaggle/input/slideandseekclasificationlandslidedetectiondataset\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Defining data paths and preloading the train and test CSV files"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_csv_path = f'{data}/Train.csv'\n",
    "test_csv_path = f'{data}/Test.csv'\n",
    "train_data_path = f'{data}/train_data/train_data'\n",
    "test_data_path = f'{data}/test_data/test_data'\n",
    "\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "print(\"Train.csv:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Visualization of class distribution (No landslide effects/after landslide)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "label_counts = train_df['label'].value_counts()\n",
    "labels = ['No Landslide', 'Landslide']\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(labels, label_counts.values, color=['skyblue', 'salmon'])\n",
    "plt.xlabel(\"Class Label\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Labels in Training Set\")\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### As can be seen from the graph, there is a strong imbalance of classes No Landslide ~ 6000 images, Landslide ~ 1400 images\n",
    "### This problem will be solved further using Focal loss"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Image loading and normalization function, as well as their visualization\n",
    "### The images consist of 12 bands (4 optical from Sentinel-2 and 8 SAR from Sentinel-1) and are 64x64 in size. For each band, a robust percentile normalization (2nd and 98th) is performed to make the pixel values in the range [0, 1] and reduce the influence of outliers that may be present in the satellite data. Normalization to 1/99th percentile and minmax scaling (commented code) were also tested, but they gave worse results."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_and_normalize_npy_image(image_id, folder_path):\n",
    "    image_path = os.path.join(folder_path, f\"{image_id}.npy\")\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "    \n",
    "    img = np.load(image_path).astype(np.float32)\n",
    "\n",
    "    normalized_img = img\n",
    "    for band in range(img.shape[2]):\n",
    "        band_data = img[:, :, band]\n",
    "        \n",
    "        p2, p98 = np.percentile(band_data, (2, 98))\n",
    "        normalized_img[:, :, band] = np.clip((band_data - p2) / (p98 - p2 + 1e-6), 0, 1)\n",
    "        \n",
    "    #     p1, p99 = np.percentile(band_data, (1, 99))\n",
    "\n",
    "    #     clipped_data = np.clip(band_data, p1, p99)\n",
    "    #     mean_val = np.mean(clipped_data)\n",
    "    #     std_val = np.std(clipped_data)\n",
    "        \n",
    "    #     if std_val > 0:\n",
    "    #         normalized_img[:, :, band] = (clipped_data - mean_val) / std_val\n",
    "    #     else:\n",
    "    #         normalized_img[:, :, band] = clipped_data - mean_val\n",
    "    \n",
    "    # for band in range(img.shape[2]):\n",
    "    #     band_data = normalized_img[:, :, band]\n",
    "    #     min_val, max_val = np.min(band_data), np.max(band_data)\n",
    "    #     if max_val > min_val:\n",
    "    #         normalized_img[:, :, band] = (band_data - min_val) / (max_val - min_val)\n",
    "    \n",
    "    return normalized_img\n",
    "\n",
    "band_descriptions = [\n",
    "    \"Red\", \"Green\", \"Blue\", \"Near Infrared\",\n",
    "    \"Descending VV (Vertical-Vertical)\", \"Descending VH (Vertical-Horizontal)\",\n",
    "    \"Descending Diff VV\", \"Descending Diff VH\",\n",
    "    \"Ascending VV (Vertical-Vertical)\", \"Ascending VH (Vertical-Horizontal)\",\n",
    "    \"Ascending Diff VV\", \"Ascending Diff VH\"\n",
    "]\n",
    "\n",
    "example_ids = train_df['ID'].sample(2,random_state = 42).values\n",
    "\n",
    "for image_id in example_ids:\n",
    "    img_normalized = load_and_normalize_npy_image(image_id, train_data_path)\n",
    "\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 12))\n",
    "    fig.suptitle(f\"Sample Image ID: {image_id} - All 12 Bands\", fontsize=16)\n",
    "\n",
    "    for band in range(12):\n",
    "        row = band // 4\n",
    "        col = band % 4\n",
    "        axes[row, col].imshow(img_normalized[:, :, band], cmap='gray')\n",
    "        axes[row, col].set_title(f\"Band {band + 1}: {band_descriptions[band]}\")\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### This is what two 12-channel images look like (the first three channels are the usual RGB, then infrared and other types of radiation produced and received by Sentinel 2."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Defining a data generator for loading data in batches with random examples from train and applying random augmentations (15 degree rotation, width and height shifts, scaling, reflections and filling in gaps at the edges.)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "class LandslideDataGenerator(Sequence):\n",
    "    def __init__(self, df, data_path, batch_size=32, augment=False, shuffle=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augment\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        if self.augment:\n",
    "            self.augmentor = ImageDataGenerator(\n",
    "                rotation_range=15,\n",
    "                width_shift_range=0.1,\n",
    "                height_shift_range=0.1,\n",
    "                shear_range=0.1,\n",
    "                zoom_range=0.1,\n",
    "                horizontal_flip=True,\n",
    "                vertical_flip=True,\n",
    "                fill_mode='reflect' \n",
    "            )\n",
    "        else:\n",
    "            self.augmentor = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_df = self.df.iloc[batch_indexes]\n",
    "        \n",
    "        batch_X = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for _, row in batch_df.iterrows():\n",
    "            image_id = row['ID']\n",
    "            label = row['label']\n",
    "           \n",
    "            image = load_and_normalize_npy_image(image_id, self.data_path)\n",
    "            batch_X.append(image)\n",
    "            batch_y.append(label)\n",
    "        \n",
    "        batch_X = np.array(batch_X, dtype=np.float32)\n",
    "        batch_y = np.array(batch_y, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "        if self.augment and self.augmentor:\n",
    "            batch_X = next(self.augmentor.flow(batch_X, batch_size=len(batch_X), shuffle=False))\n",
    "        return batch_X, batch_y\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "folder_path = '/kaggle/input/slideandseekclasificationlandslidedetectiondataset/train_data/train_data/'\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_df_split, val_df_split = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "train_gen = LandslideDataGenerator(train_df_split, folder_path, batch_size=batch_size, augment=True, shuffle=True)\n",
    "val_gen = LandslideDataGenerator(val_df_split, folder_path, batch_size=batch_size, augment=False, shuffle=False)\n",
    "\n",
    "X_batch, y_batch = train_gen[0]\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### As we can see in the X batch there are 16 images 64x64 with 12 channels and in the label(y) batch there are 16 classes corresponding to the images"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Output y_batch"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y_batch"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Definition of Focal Loss\n",
    "### Focal Loss was designed to address class imbalance in problems where one class is significantly dominant over another (like in landslides). It modifies the standard cross-entropy loss by decreasing the weight of \"easy\" (well-classified) examples and increasing the weight of \"hard\" (badly-classified) examples, especially for the minority class.\n",
    "\n",
    "### gamma (focus parameter): controls how much \"easy\" examples are suppressed. Higher gamma -> stronger focus on hard ones.\n",
    "\n",
    "### alpha (balancing factor): For a rare positive class, alpha is usually set high (e.g. 0.75 or 0.9 depending on the degree of imbalance). In the current implementation, alpha=None by default, which means automatic weight calculation (based on the number of class examples in the train)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def focal_loss(gamma=2.0, alpha=0.75):\n",
    "    \"\"\"\n",
    "    Focal Loss for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "        gamma (float): Focusing parameter; typically set to 2.0.\n",
    "        alpha (float): Balancing factor; typically set to 0.25.\n",
    "\n",
    "    Returns:\n",
    "        Binary Focal Loss function.\n",
    "    \"\"\"\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "        if alpha is None:\n",
    "\n",
    "            pos_weight = K.mean(1 - y_true)\n",
    "            neg_weight = K.mean(y_true)\n",
    "            alpha_t = tf.where(K.equal(y_true, 1), pos_weight, neg_weight)\n",
    "        else:\n",
    "            alpha_t = tf.where(K.equal(y_true, 1), alpha, 1 - alpha)\n",
    "\n",
    "        p_t = tf.where(K.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "\n",
    "        fl = -alpha_t * K.pow(1 - p_t, gamma) * K.log(p_t)\n",
    "        return K.mean(fl)\n",
    "    \n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Standard metrics Precision, Recall and F1-score"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def precision_m(y_true, y_pred):\n",
    "    y_pred_bin = tf.cast(y_pred >= 0.5, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    true_positives = tf.reduce_sum(y_true * y_pred_bin)\n",
    "    predicted_positives = tf.reduce_sum(y_pred_bin)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    y_pred_bin = tf.cast(y_pred >= 0.5, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    true_positives = tf.reduce_sum(y_true * y_pred_bin)\n",
    "    possible_positives = tf.reduce_sum(y_true)\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Then more than 50 experiments were conducted on various architectures: simple CNN models of different depths, with different activation functions (RelU LeakyReLU, ELU) with different parameters for dropout and convolution sizes, with different hyperparameters lr, batch_size, etc. ### F1 plot on the validation set of all Cnn experiments:\n",
    "![all_cnn](https://github.com/MakarRybkin/Slide_and_seek_classification/raw/master/images_for_report/all_cnn_f1.png)\n",
    "### With this approach, we were able to achieve f1 = 0.8054\n",
    "### F1 plot on the validation set of the best Cnn experiment:\n",
    "![best_cnn.png](https://github.com/MakarRybkin/Slide_and_seek_classification/raw/master/images_for_report/best_cnn.png)\n",
    "### An example of one of the CNN models can be seen in the commented code below"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# model = Sequential([\n",
    "#\n",
    "#     Input(shape=X_batch.shape[1:]),\n",
    "#     Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "#     BatchNormalization(),\n",
    "#     Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "#     MaxPooling2D((2, 2)),\n",
    "#     Dropout(0.1),\n",
    "    \n",
    "#\n",
    "#     Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "#     BatchNormalization(),\n",
    "#     Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "#     MaxPooling2D((2, 2)),\n",
    "#     Dropout(0.1),\n",
    "    \n",
    "#\n",
    "#     Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "#     BatchNormalization(),\n",
    "#     Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "#     MaxPooling2D((2, 2)),\n",
    "#     Dropout(0.15),\n",
    "    \n",
    "#\n",
    "#     Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "#     BatchNormalization(),\n",
    "#     Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "#     GlobalAveragePooling2D(),  # Better than Flatten for reducing parameters\n",
    "#     Dropout(0.15),\n",
    "    \n",
    "#\n",
    "#     Dense(64, activation='relu'),\n",
    "#     BatchNormalization(),\n",
    "#     Dropout(0.3),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# optimizer = AdamW(\n",
    "#     learning_rate=0.005,  \n",
    "#     weight_decay=0.01,\n",
    "#     beta_1=0.9,\n",
    "#     beta_2=0.999\n",
    "# )\n",
    "\n",
    "# model.compile(\n",
    "#         optimizer=optimizer,\n",
    "#         loss=focal_loss(gamma=2.0, alpha=None),  # Auto-balanced\n",
    "#         metrics=[f1_m, precision_m, recall_m]\n",
    "#     )\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### After that, experiments were also conducted with different parameters and hyperparameters for more complex ready-made models (EfficientNet(B0 - B7) EfficientNetV2L, different depths of ResNet and DenseNet followed by the head of the model, consisting of GlobalAveragePooling2D() to reduce the dimensionality after the output and several layers of FCNN\n",
    "\n",
    "### The f1 graph on the validation set of all experiments with these architectures:\n",
    "\n",
    "![all_ef_res_dense.png](https://github.com/MakarRybkin/Slide_and_seek_classification/raw/master/images_for_report/all_ef_res_dense.png)\n",
    "### As a result, the best models were EfficientNet B4 (best f1 ~ 0.8389), ResNet50 (best f1 ~ 0.838) DenseNet_169 (best f1 ~ 0.841)\n",
    "\n",
    "### f1 plot on the validation set for the best experiment with EfficientNetB4 :\n",
    "![best_eff_b4.png](https://github.com/MakarRybkin/Slide_and_seek_classification/raw/master/images_for_report/best_eff_b4.png)\n",
    "\n",
    "### f1 plot on the validation set for the best experiment with ResNet50 :\n",
    "![best_resnet.png](https://github.com/MakarRybkin/Slide_and_seek_classification/raw/master/images_for_report/best_resnet.png)\n",
    "\n",
    "### f1 plot on the validation set for the best experiment with DenseNet169:\n",
    "![best_densenet.png](https://github.com/MakarRybkin/Slide_and_seek_classification/raw/master/images_for_report/best_densenet.png)\n",
    "### The commented code for training one of the models from this series can be seen below"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from tensorflow.keras.applications import EfficientNetB3 , EfficientNetV2L , DenseNet169, ResNet50\n",
    "# from tensorflow.keras.layers import LeakyReLU, ELU , ReLU\n",
    "# input_shape = X_batch.shape[1:] \n",
    "\n",
    "# base_efficientnet = EfficientNetB3(weights=None, include_top=False, input_shape=input_shape)\n",
    "\n",
    "# # densnet_169 = DenseNet169(weights=None, include_top=False, input_shape=input_shape)\n",
    "\n",
    "# # resnet50 = ResNet50(weights=None, include_top=False, input_shape=input_shape)\n",
    "\n",
    "# model = Sequential([\n",
    "#     base_efficientnet,\n",
    "#     GlobalAveragePooling2D(),\n",
    "    \n",
    "#     Dense(512), \n",
    "#     BatchNormalization(),\n",
    "#     LeakyReLU(alpha = 0.01), \n",
    "#     Dropout(0.3),\n",
    "    \n",
    "#     Dense(256),\n",
    "#     BatchNormalization(),\n",
    "#     LeakyReLU(alpha = 0.01),\n",
    "#     Dropout(0.2),\n",
    "    \n",
    "#     Dense(64),\n",
    "#     BatchNormalization(),\n",
    "#     LeakyReLU(alpha = 0.01),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "\n",
    "# optimizer_efficientnet = tf.keras.optimizers.AdamW(\n",
    "#     learning_rate=0.001,\n",
    "#     weight_decay=0.01,\n",
    "#     beta_1=0.9,\n",
    "#     beta_2=0.999\n",
    "# )\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=optimizer_efficientnet,\n",
    "#     loss=focal_loss(gamma=2.0, alpha=None), \n",
    "#     metrics=[f1_m, precision_m, recall_m] \n",
    "# )\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Definition of constants and hyperparameters\n",
    "\n",
    "### As a result of previous experiments, the best hyperparameters and architectures (from those studied) were identified; they will be used below for combining into an ensemble"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow.keras.applications import EfficientNetB5 , EfficientNetV2L , DenseNet169, ResNet50\n",
    "from tensorflow.keras.layers import LeakyReLU, ELU , ReLU\n",
    "MODEL_TYPES_FOR_ENSEMBLE = [\n",
    "    'EfficientNetB5',\n",
    "    'DenseNet169',\n",
    "    'ResNet50'\n",
    "]\n",
    "NUM_RUNS_PER_MODEL_TYPE = 2\n",
    "BASE_RANDOM_SEED = 100\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE_PHASE1 = 16\n",
    "EPOCHS_PHASE1 = 30\n",
    "BATCH_SIZE_PHASE2 = 32\n",
    "EPOCHS_PHASE2_MAX = 70\n",
    "PATIENCE_EARLY_STOPPING = 25\n",
    "PATIENCE_REDUCE_LR = 6\n",
    "MIN_LR = 1e-8\n",
    "DROPOUT_RATE_DENSE1 = 0.2\n",
    "DROPOUT_RATE_DENSE2 = 0.15\n",
    "DROPOUT_RATE_DENSE3 = 0.15\n",
    "\n",
    "COMET_PROJECT_NAME = \"landslide-ensemble-custom-loop\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Function to create and compile a single model (will be used below for each model in the ensemble). Depending on the model_type (EfficientNetB5, DenseNet169 or ResNet50), the corresponding architecture is loaded from tensorflow.keras.applications. weights=None specifies that the models are loaded without pre-trained ImageNet weights. optimizer and compiler for the model.\n",
    "\n",
    "### The following layers are added to the base model for classification:\n",
    "#### GlobalAveragePooling2D(): Reduces the dimensionality of spatial features to a single vector by averaging the values over the width and height.\n",
    "\n",
    "#### Dense (fully connected layers): Two fully connected layers (256 and 64 neurons) with LeakyReLU activation functions for nonlinearity.\n",
    "\n",
    "#### BatchNormalization(): Normalizes activations to speed up training and improve stability.\n",
    "\n",
    "#### Dropout(): Uses neuron pruning to prevent overfitting.\n",
    "\n",
    "#### Final Dense(1, activation='sigmoid'): Single neuron output layer with sigmoid activation for binary classification (output in range [0, 1], interpreted as probability of landslide).\n",
    "\n",
    "#### Uses tf.keras.optimizers.AdamW. This is a variant of Adam optimizer that explicitly takes weight decay into account, which helps prevent overfitting.\n",
    "\n",
    "### Compiling the model: The model is compiled with:\n",
    "\n",
    "#### optimizer: AdamW defined above.\n",
    "\n",
    "#### loss: Custom focal_loss function to deal with class imbalance.\n",
    "\n",
    "#### metrics: Custom metrics f1_m, precision_m, recall_m to evaluate performance."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_compiled_model(model_type, input_shape):\n",
    "    base_model = None\n",
    "    if model_type == 'EfficientNetB5':\n",
    "        base_model = EfficientNetB5(weights=None, include_top=False, input_shape=input_shape)\n",
    "    elif model_type == 'DenseNet169':\n",
    "        base_model = DenseNet169(weights=None, include_top=False, input_shape=input_shape)\n",
    "    elif model_type == 'ResNet50':\n",
    "        base_model = ResNet50(weights=None, include_top=False, input_shape=input_shape)\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "\n",
    "        Dense(256),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.01), \n",
    "        Dropout(DROPOUT_RATE_DENSE2),\n",
    "        \n",
    "        Dense(64),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.01), \n",
    "        Dropout(DROPOUT_RATE_DENSE3),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=focal_loss(gamma=2.0, alpha=None),\n",
    "        metrics=[f1_m, precision_m, recall_m]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Main model ensemble training loop\n",
    "\n",
    "### Initialization: Defines variables to store paths to trained models and metadata.\n",
    "\n",
    "#### Double loop: The code loops through each model_type (EfficientNetB5, DenseNet169, ResNet50) and for each type performs NUM_RUNS_PER_MODEL_TYPE (in this case 2) independent training runs. This creates diversity in the ensemble.\n",
    "\n",
    "#### Reproducibility: Each run sets a new current_seed for tf.random, np.random, and os.environ['PYTHONHASHSEED'] to ensure that random initialization and operations are unique, but it is still possible to replay each individual run given its seed.\n",
    "\n",
    "#### Comet ML integration:\n",
    "\n",
    "#### A new comet_ml.Experiment object is created before each new experiment.\n",
    "\n",
    "#### Sets the name of the experiment that will be displayed in the Comet ML dashboard.\n",
    "\n",
    "#### This allows you to track metrics, loss, and other data for each model run separately.\n",
    "\n",
    "#### Two-Phase Training: Each model is trained in two phases:\n",
    "\n",
    "#### Phase 1: Train with BATCH_SIZE_PHASE1=16. Smaller batches can help converge faster at the beginning of training.\n",
    "\n",
    "#### Phase 2: Continue training with BATCH_SIZE_PHASE2=32. Larger batches can help provide a more stable gradient and better generalization in the later stages.\n",
    "\n",
    "#### Callbacks: The same callbacks are used for both training phases:\n",
    "\n",
    "#### ModelCheckpoint: Saves the best version of the model based on the val_f1_m metric (F1 on the validation set). save_best_only=True ensures that only the best model is saved.\n",
    "\n",
    "#### EarlyStopping: Stops training if val_f1_m does not improve within PATIENCE_EARLY_STOPPING=25 epochs. restore_best_weights=True loads the best epoch weights after stopping.\n",
    "\n",
    "#### ReduceLROnPlateau: Reduces the learning rate if val_f1_m does not improve within PATIENCE_REDUCE_LR=6 epochs. This helps the model escape from local minima.\n",
    "\n",
    "#### Model training: model.fit() runs the training process using the generated data generators and callbacks.\n",
    "\n",
    "#### Saving models: After both phases of training (or early stopping) are complete, the best version of each trained model is saved to disk as h5.keras or keras in the ensemble_models_custom folder. The paths and metadata of the models are recorded for later use in the ensemble.\n",
    "\n",
    "#### Ending a Comet ML experiment: experiment.end() closes the current Comet ML experiment, saving all logs."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "input_shape = X_batch.shape[1:] \n",
    "\n",
    "trained_model_paths = [] \n",
    "model_metadata = []      \n",
    "\n",
    "total_runs = len(MODEL_TYPES_FOR_ENSEMBLE) * NUM_RUNS_PER_MODEL_TYPE\n",
    "run_counter = 0\n",
    "\n",
    "for model_type in MODEL_TYPES_FOR_ENSEMBLE:\n",
    "    for i in range(NUM_RUNS_PER_MODEL_TYPE):\n",
    "        run_counter += 1\n",
    "        print(f\"\\n--- Обучение модели {model_type} (Запуск {i+1}/{NUM_RUNS_PER_MODEL_TYPE}) ---\")\n",
    "\n",
    "        current_seed = BASE_RANDOM_SEED + run_counter * 10\n",
    "        tf.random.set_seed(current_seed)\n",
    "        np.random.seed(current_seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(current_seed)\n",
    "\n",
    "        if 'experiment' in globals() and isinstance(globals()['experiment'], comet_ml.Experiment):\n",
    "            if not globals()['experiment'].ended:\n",
    "                globals()['experiment'].end()\n",
    "        experiment = comet_ml.Experiment(\n",
    "            api_key=COMET_API_KEY,\n",
    "            project_name=\"Classification for Landslide Detection Ensamble\", \n",
    "            auto_output_logging=\"simple\"\n",
    "        )\n",
    "        experiment.flush()\n",
    "        experiment_name = f\"{model_type}_run_{i+1}_seed_{current_seed}\"\n",
    "        experiment.set_name(experiment_name)\n",
    "\n",
    "        print(f\"--- Фаза 1: Обучение с Batch Size {BATCH_SIZE_PHASE1} на {EPOCHS_PHASE1} эпох ---\")\n",
    "        train_gen_phase1 = LandslideDataGenerator(\n",
    "            train_df_split, folder_path, batch_size=BATCH_SIZE_PHASE1, augment=True, shuffle=True)\n",
    "        val_gen_phase1 = LandslideDataGenerator(\n",
    "            val_df_split, folder_path, batch_size=BATCH_SIZE_PHASE1, augment=False, shuffle=False)\n",
    "\n",
    "        model = create_compiled_model(model_type, input_shape)\n",
    "\n",
    "        callbacks_phase1 = [\n",
    "            ModelCheckpoint(\n",
    "                f\"best_model{experiment_name}.h5.keras\",\n",
    "                monitor='val_f1_m',\n",
    "                mode='max',\n",
    "                save_best_only=True,\n",
    "                verbose=3\n",
    "            ),\n",
    "            EarlyStopping(\n",
    "                monitor='val_f1_m', \n",
    "                mode='max',\n",
    "                patience=PATIENCE_EARLY_STOPPING,\n",
    "                verbose=2,\n",
    "                restore_best_weights=True \n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_f1_m',\n",
    "                mode='max',\n",
    "                factor=0.4,\n",
    "                patience=PATIENCE_REDUCE_LR,\n",
    "                min_lr=MIN_LR,\n",
    "                verbose=2\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        history_phase1 = model.fit(\n",
    "            train_gen_phase1,\n",
    "            epochs=EPOCHS_PHASE1,\n",
    "            validation_data=val_gen_phase1,\n",
    "            callbacks=callbacks_phase1,\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- Фаза 2: Продолжение обучения с Batch Size {BATCH_SIZE_PHASE2} на макс. {EPOCHS_PHASE2_MAX} эпох ---\")\n",
    "        train_gen_phase2 = LandslideDataGenerator(\n",
    "            train_df_split, folder_path, batch_size=BATCH_SIZE_PHASE2, augment=True, shuffle=True)\n",
    "        val_gen_phase2 = LandslideDataGenerator(\n",
    "            val_df_split, folder_path, batch_size=BATCH_SIZE_PHASE2, augment=False, shuffle=False)\n",
    "\n",
    "        callbacks_phase2 = [\n",
    "            ModelCheckpoint(\n",
    "                f\"best_model{experiment_name}.h5.keras\",\n",
    "                monitor='val_f1_m',\n",
    "                mode='max',\n",
    "                save_best_only=True,\n",
    "                verbose=3\n",
    "            ),\n",
    "            EarlyStopping(\n",
    "                monitor='val_f1_m',\n",
    "                mode='max',\n",
    "                patience=PATIENCE_EARLY_STOPPING,\n",
    "                restore_best_weights=True,\n",
    "                verbose=2\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_f1_m',\n",
    "                mode='max',\n",
    "                factor=0.4,\n",
    "                patience=PATIENCE_REDUCE_LR,\n",
    "                min_lr=MIN_LR,\n",
    "                verbose=2\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        history_phase2 = model.fit(\n",
    "            train_gen_phase2,\n",
    "            epochs=EPOCHS_PHASE2_MAX,\n",
    "            validation_data=val_gen_phase2,\n",
    "            callbacks=callbacks_phase2,\n",
    "        )\n",
    "\n",
    "        model_save_dir = 'ensemble_models_custom'\n",
    "        os.makedirs(model_save_dir, exist_ok=True)\n",
    "        model_save_path = os.path.join(model_save_dir, f'{experiment_name}.keras')\n",
    "        model.save(model_save_path, save_format='keras')\n",
    "        print(f\"Модель {model_type} (Запуск {i+1}) сохранена в {model_save_path}\")\n",
    "\n",
    "        trained_model_paths.append(model_save_path)\n",
    "        model_metadata.append({'type': model_type, 'seed': current_seed, 'path': model_save_path})\n",
    "\n",
    "        experiment.end()\n",
    "\n",
    "print(\"\\n--- Обучение всех моделей ансамбля завершено ---\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### F1 plot on the validation set for all 6 models from the ensemble (2 each EfficientnetB4, ResNet50, DenseNet169)\n",
    "![ensamble.png](https://github.com/MakarRybkin/Slide_and_seek_classification/raw/master/images_for_report/ensamble.png)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Getting predictions for each trained model from the ensemble and averaging the responses"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow.keras.models import  load_model\n",
    "all_predictions = []\n",
    "\n",
    "print(\"\\n--- Получение предсказаний каждой обученной моделью ---\")\n",
    "for model_path in trained_model_paths:\n",
    "    print(f\"Загрузка модели: {model_path}\")\n",
    "\n",
    "    model = load_model(model_path,compile=False)\n",
    "\n",
    "    preds = model.predict(val_gen, verbose=1)\n",
    "    all_predictions.append(preds)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "ensemble_probabilities = np.mean(all_predictions, axis=0) \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Definition of the data generator for the test sample (LandslideTestGenerator). Just like LandslideDataGenerator, it loads data in batches"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LandslideTestGenerator(Sequence):\n",
    "    def __init__(self, df, data_path, batch_size=32):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_df = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_X = []\n",
    "\n",
    "        for _, row in batch_df.iterrows():\n",
    "            image_id = row['ID']\n",
    "            image = load_and_normalize_npy_image(image_id, self.data_path)\n",
    "            batch_X.append(image)\n",
    "\n",
    "        return np.array(batch_X)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Performing predictions on a test sample and creating a file for submission"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "test_gen = LandslideTestGenerator(test_df, test_data_path, batch_size=32)\n",
    "\n",
    "all_test_predictions = []\n",
    "\n",
    "print(\"Making predictions with each trained model on the Test set:\")\n",
    "for model_path in trained_model_paths:\n",
    "    print(f\"Loading model for test predictions: {model_path}\")\n",
    "    \n",
    "    model = load_model(model_path, compile=False)\n",
    "\n",
    "    preds_on_test = model.predict(test_gen, verbose=1)\n",
    "    all_test_predictions.append(preds_on_test)\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "all_test_predictions = np.array(all_test_predictions)\n",
    "ensemble_test_probabilities = np.mean(all_test_predictions, axis=0)\n",
    "\n",
    "y_test_pred = (ensemble_test_probabilities > 0.5).astype(int)\n",
    "\n",
    "unique, counts = np.unique(y_test_pred, return_counts=True)\n",
    "prediction_counts = dict(zip(unique, counts))\n",
    "print(\"Prediction counts:\", prediction_counts)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'label': y_test_pred.flatten()\n",
    "})\n",
    "submission_df.to_csv(f'Submission_File{experiment_name}.csv', index=False)\n",
    "print(f\"Sample submission file created as 'Submission_File{experiment_name}.csv'.\")\n"
   ]
  }
 ]
}
